# -*- coding: utf-8 -*-
"""Stout_casestudy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bImBprwLdXjCnPE3st6hhcWI81Dot-fD

# Stout Take-home Challenge

# Case Study 1

# Executive Summary
In this report, I conducted an exploratory data analysis on the transaction data and identify 4 underlying data issues. I then took actions to further address the issues, and build 3 machine learning models (Logistic Regression, Random Forest & XGBoost) to predict default rate. 

It turned out that Random Forest performs the best with 98% out-of-sample prediction accuracy.

## Exploratory analysis

### Data description

This is a data set with 6362620 rows of syhthetic mobile money transactions and no null value. It has 11 variables with 8 numeric variables and 5 categorical vatiables, and none of the variables follows normal distribution.

### Data issues
The data set have the following issues:
1. Imbalance data issue
  99.87% of the data is not fraud, while only 0.13% of the data is fraud.
2. Fraud detection accuracy issue
  There are 6362604 fraud data, while only 16 of them are flagged as fraud.
3. Data correlation
  There are high correlation between two pairs of variables: oldBalanceOrg and newBalanceOrg, oldBalanceDest and newBalanceDest.
4. Data distribution
  For example,the "amount" data is right skewed.

## Data Cleansing & Model Building
In this section, I performed data cleansing (dropped highly correlated independent variables and irrelevant variables, turned numeric data into categorical data used SMOTE to due with data imbalance issue), built 3 models and evaluated their performance against Accuracy, Precision, Recall and F1.

## Next Step
There are 3 steps I would take for model enhancement if given more time:
First, perform parameter tuning to further improve existing model performance
Second, conduct feature importance for Random Forest model to get a better understanding of what features contributes the most to determining fraud
Third, explore more machine learning models to see if the performance can be better.
"""

pip install nbconvert

import pandas as pd
import numpy as np
from collections import Counter
import statsmodels.api as sm

# Data Preparation and visualization
import seaborn as sns
from matplotlib import pyplot as plt
from imblearn.under_sampling import RandomUnderSampler  
from sklearn.preprocessing import StandardScaler   
from sklearn.manifold import TSNE
import matplotlib.patches as mpatches
from pylab import rcParams

# Model Selection & model traning 
from sklearn.model_selection import GridSearchCV
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegressionCV
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

from sklearn.preprocessing import StandardScaler
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import plot_confusion_matrix

from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_curve, f1_score, auc, confusion_matrix
import time
from sklearn.metrics import roc_auc_score
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from xgboost import XGBClassifier
from sklearn.decomposition import PCA
from sklearn import metrics   
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split


# Expand the size of terminal window to display all rows
pd.set_option('display.max_columns', 500)

from google.colab import drive
drive.mount('/content/drive/')

# Commented out IPython magic to ensure Python compatibility.
# %cd '/content/drive/MyDrive/Stout_takehome'

!unzip '/content/drive/MyDrive/Stout_takehome/archive.zip'

# Load data into dataframe
data = pd.read_csv('PS_20174392719_1491204439457_log.csv')
data.head()

data.describe()

data.info()

data.dtypes

data['type'].unique()

## check for missing data
data.isnull().sum()

data.count()

## check for data balance
pct = (data.isFraud.values == 0).mean()

plot = sns.barplot(['is Not Fraud','is Fraud'],[pct*100, (1-pct)*100])
plot.set(ylabel = 'Percentage')
plt.title("Distribution of Fraud/Is Not fraud") 

for i in range(2):
    a = plot.patches[i]
    height = a.get_height()
    value = abs(pct - i)
    plot.text(a.get_x() + a.get_width()/2., height + 0.5, round(value * 100, 2), ha = "center")

"""There appears to be a data imbalance issue: 99.87% of the data is not fraud, while only 0.13% of the data is fraud.

I would assume the 'isFlaggedFraud' variable as current fraud detection outcome. From the data, I can see that the current fraud detection accuracy is very low. This would pose another data issue.

## Data Visualization
Generate a minimum of 5 unique visualizations
"""

data.shape
fig = plt.figure(figsize=(12,2))
#looking at correlations between each of the two variables
data_matrix = data[data.columns[:-2]].corr()
plt.subplots(figsize=(25, 15))
plt.title('Correlation between each of the two variables')
sns.heatmap(data_matrix,annot=True)
plt.show()

"""We can see that there are high correlation between two pairs of variables: oldBalanceOrg and newBalanceOrg, oldBalanceDest and newBalanceDest."""

## Split the dataset based on whether the user has download the app. "1" as "download" and "0" as "not dowload"
t0 = data.loc[data['isFraud'] == 0]
t1 = data.loc[data['isFraud'] == 1]

import matplotlib.patches as mpatches
####### check ######
var = ['step','amount']

i = 0

sns.set_style('whitegrid')
plt.figure()
fig, ax = plt.subplots(1,4,figsize=(16,4))

for feature in var:
    i += 1
    plt.subplot(1,2,i)
    sns.kdeplot(t1[feature], bw_method=0.5)
    sns.kdeplot(t0[feature], bw_method=0.5, color = 'Orange')
    plt.ylabel('Density plot', fontsize=12)
    plt.xlabel(feature, fontsize=12)
    locs, labels = plt.xticks()
    plt.tick_params(axis='both', which='major', labelsize=12)
plt.show();

# check the distribution of the "amount" data type
amount = [data['amount'].values]
plt.figure(figsize=(12,8))
sns.distplot(amount)
plt.show()

"""We can see that the "amount" variable is right skewed. """

# check the number of transactions for different type
data_col_type = data['type']
plt.figure()
data_col_type.hist()
plt.ylabel('counts')
plt.xticks(rotation = 75)
plt.title('Number of Transactions for Different Type')
plt.show()

data['isFraud'].value_counts()
data['isFlaggedFraud'].value_counts()

"""We also learn that there are uneven amount of transaction type, with payment being the highest.

## Data cleansing & manipulation

Conduct data cleansing & manipulation and create a feature set. Perform prediction of fraudulent transactions using at least 2 algorithms. Describe any data cleansing that must be performed.
"""

#Drop features within a pair that has high correlation - mitigate multicollinearity. Parameter to tune: cor value 0.7
features_to_drop = set()
feature_list = [x for x in data_matrix]
n = len(feature_list)
for i in range(n):
    for j in range(i+1, n):
        f1 = feature_list[i]
        f2 = feature_list[j]
        if abs(data_matrix[f1][f2]) > 0.7: features_to_drop.add(f1)

features_to_drop

## drop independent variables with high correlation
data.drop(features_to_drop, inplace = True, axis = 1)

## drop identifiable information, which are not useful for modeling
data.drop(['nameDest', 'nameOrig'], inplace = True, axis = 1)

data.dtypes

"""### Split Test Train

As we can see in the previous section, variable "type" a categorical. Next, I will use get_dummies() to transform it into numerical one and conduct a train-test split before modeling.

Furthermore, I will solve the data imbalance issue through the SMOTE pacakage.
"""

X=data.drop(columns=['isFraud'])
Y = data['isFraud']

## Use Onehotencoding to turn categorical variables into numerical variables
X=pd.get_dummies(X, columns = (X.select_dtypes(include = ['object']).columns))

# make the unbalaned data balanced
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
# define pipeline
over = SMOTE(sampling_strategy=0.1)
under = RandomUnderSampler(sampling_strategy=0.5)
steps = [('o', over), ('u', under)]
pipeline = Pipeline(steps=steps)
# transform the dataset
x, y = pipeline.fit_resample(X,Y)
counter = Counter(y)
print(counter)

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)

"""## Model Building

### Logistic Regression
"""

#Fit logistic regression
logit = LogisticRegression()
logit.fit(X_train, y_train)

# Out of sample evaluation
y_pred = logit.predict(X_test)
confusionMatrix = confusion_matrix(y_test, y_pred)
plot_confusion_matrix(logit, X_test, y_test)  
print('Precision:', metrics.precision_score(y_test, y_pred))
print('Recall   :', metrics.recall_score(y_test, y_pred))
print('Accuracy :', metrics.accuracy_score(y_test, y_pred))
print('F1 score :', metrics.f1_score(y_test, y_pred))

"""### Random Forest"""

rf = RandomForestClassifier()
rf.fit(X_train, y_train)

preds = rf.predict(X_test)
confusionMatrix = confusion_matrix(y_test, preds)
plot_confusion_matrix(rf, X_test, y_test)  
print('Precision:', metrics.precision_score(y_test, preds))
print('Recall   :', metrics.recall_score(y_test, preds))
print('Accuracy :', metrics.accuracy_score(y_test, preds))
print('F1 score :', metrics.f1_score(y_test, preds))

# rf = RandomForestClassifier(criterion="gini")
# #Use GridSearchCV for parameter tuning
# parameters = {"n_estimators"    : [100, 200, 500, 800, 1000] ,
#                "min_samples_leaf" : [ 3, 5, 7, 10, 15, 20, 25, 30],
#                "max_depth" : [ 3, 5, 7, 10, 15, 20]}
# model_rf = GridSearchCV(rf, parameters, cv = 5, n_jobs=-1, verbose=1)
# model_rf.fit(X_train, y_train)
# print('Best Model:', model_rf.best_estimator_)
# print('Best score:', model_rf.best_score_)

# #Fit logistic regression
# logit = LogisticRegression(max_iter = 100)
# #Use GridSearchCV for parameter tuning
# parameters = {'C': [0.01, 0.1, 1.0, 10, 100]}
# model_logit = GridSearchCV(logit, parameters, cv = 5, n_jobs=-1, verbose=1)
# model_logit.fit(X_train, y_train)
# print('Best Model:', model_logit.best_estimator_)
# print('Best score:', model_logit.best_score_)

"""### XGBoost"""

xgboost = XGBClassifier()
xgboost.fit(X_train, y_train)

preds = xgboost.predict(X_test)
confusionMatrix = confusion_matrix(y_test, preds)
plot_confusion_matrix(xgboost, X_test, y_test)  
print('Precision:', metrics.precision_score(y_test, preds))
print('Recall   :', metrics.recall_score(y_test, preds))
print('Accuracy :', metrics.accuracy_score(y_test, preds))
print('F1 score :', metrics.f1_score(y_test, preds))

# xgboost = XGBClassifier()
# #Use GridSearchCV for parameter tuning
# parameters = {"learning_rate"    : [0.10, 0.15] ,
#                "max_depth"        : [ 3, 4, 6, 10, 15],
#                "min_child_weight" : [ 3, 5, 7, 12],
#                "gamma"            : [ 0.1, 0.4],
#                "colsample_bytree" : [ 0.3, 0.5]}
# model_xgboost = GridSearchCV(xgboost, parameters, cv = 3, n_jobs=-1, verbose=1)
# model_xgboost.fit(X_train, y_train)
# print('Best Model:', model_xgboost.best_estimator_)
# print('Best score:', model_xgboost.best_score_)

"""If given more time, I will perform parameter tuning (as shown in the commented sample code) to further improve model performance and explore more models.

# Case Study 2

## Observations

There are a couple of ineresting observations through the analysis. 

1. Overall the company did a good job in customer retention and new customer acquisition from 2015 - 2017. Its customer retention rate is 97% and the number of new customers are 1.5 times more than lost customers.

2. The company sees a decrease of revenue in 2016 fllowed by a bounce-back in 2017. The revenue of existing customers almost remained the same, but new customer revenue growth was strong over the 3 year period. 

3. Although exiting customer revenue still determined the trend of total revenue, it looks like new customer growth had a significant contribution to the revenue increase in 2017. It implies that the company should spend more resources in customer acquisition to look for growth opportunities.
"""

import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt
import seaborn as sns
from pandas import DataFrame

df = pd.read_csv('casestudy.csv')
df.head()

df['year'].value_counts()

"""### The sum of revenue for each year"""

df.groupby('year')['net_revenue'].sum()

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
years = ['2015','2016','2017']
revenue =[29036749.19, 25730943.59, 31417495.03]
ax.bar(years, revenue)
plt.title('Total Revenue for Each Year')
plt.show()

"""Total revenue decreased in 2016 but reaches 3-year high.

### New Customer Revenue 
e.g. new customers not present in previous year only
"""

## filter out 2015 customer 
cus_2015 = df[df['year'] == 2015]
cus_2016 = df[df['year'] == 2016]
cus_2017 = df[df['year'] == 2017]

new_revenue_2016 = cus_2016[~cus_2016['customer_email'].isin(cus_2015['customer_email'])]

new_revenue_2016['net_revenue'].sum()

## Total revenue of new customer in 2017 
new_revenue_2017 = cus_2017[~cus_2017['customer_email'].isin(cus_2015['customer_email'])
                            & ~cus_2017['customer_email'].isin(cus_2016['customer_email'])]

new_revenue_2017['net_revenue'].sum()

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
years = ['2016','2017']
revenue =[18245491.01, 28676607.64]
ax.bar(years, revenue)
plt.title('New Customer Revenue')
plt.show()

(28676607.64 - 18245491.01)/18245491.01

"""There's a 57% revenue increase bought by new customers from 2016 to 2017

### Existing Customer Growth. 

To calculate this, use the Revenue of existing customers for current year â€“(minus) Revenue of existing customers from the previous year
"""

## filter out customer in 2017 who are also in 2016 
exist_2017 = cus_2017[cus_2017['customer_email'].isin(cus_2016['customer_email'])]['net_revenue'].sum()

## Calculate the net growth 
exist_2016 = cus_2016[cus_2016['customer_email'].isin(cus_2017['customer_email'])]['net_revenue'].sum()

## Net revenue growth of existing customer in 2016 
net_growth_2016 = exist_2017 - exist_2016
net_growth_2016

## filter out customer existing in 2016 and 2015 
exist_2016_new = cus_2016[cus_2016['customer_email'].isin(cus_2015['customer_email'])]['net_revenue'].sum()
exist_2015 = cus_2015[cus_2015['customer_email'].isin(cus_2016['customer_email'])]['net_revenue'].sum()

## Net revenue growth of existing customer from 2015 to 2016 (2016 - 2015 )
net_growth_2015 = exist_2016_new - exist_2015
net_growth_2015

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
years = ['2015-2016','2016-2017']
revenue =[20611.340000000317, 20335.459999999963]
ax.bar(years, revenue)
plt.title('Existing Customer Growth')
plt.show()

"""Existing customer growth hasn't changed much in the 3-year period.

## Revenue lost from attrition

### Existing Customer Revenue Current Year
"""

revenue_year = [cus_2015['net_revenue'].sum(),cus_2016['net_revenue'].sum(),cus_2017['net_revenue'].sum()]
revenue_year

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
years = ['2015','2016','2017']
revenue =[29036749.189999994, 25730943.59, 31417495.030000016]
ax.bar(years, revenue)
plt.title('Existing Customer Revenue Current Year')
plt.show()

"""The existing customer revenue decreased in 2016 but reached to the highest in 2017. The pattern is similar to the total revenue. Therefore, it's reasonable to make an assumption that existing customers are the main driver for revenue at the company from 2015 to 2017, given no other factors with significant changes invovled.

### Existing Customer Revenue Prior Year

#### Total Customers Current Year
"""

print('total num of customers in 2015:',cus_2015['customer_email'].nunique())
print('total num of customers in 2016:',cus_2016['customer_email'].nunique())
print('total num of customers in 2017:',cus_2017['customer_email'].nunique())

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
years = ['2015','2016','2017']
revenue =[231294, 204646, 249987]
ax.bar(years, revenue)
plt.title('Total Customers Current Year')
plt.show()

"""#### Total Customers Previous Year

- Total customers previous to 2015 is 0
"""

print('Total Customers Previous Year 2016: ',df[df['year'] < 2016]['customer_email'].nunique())
print('Total Customers Previous Year 2017: ',df[df['year'] < 2017]['customer_email'].nunique())

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
years = ['<2016','<2017']
revenue =[231294, 376356]
ax.bar(years, revenue)
plt.title('Total Customers Previous Year')
plt.show()

"""### New Customers"""

new_cus_2016 = cus_2016[~cus_2016['customer_email'].isin(cus_2015['customer_email'])]
new_cus_2017 = cus_2017[~cus_2017['customer_email'].isin(cus_2016['customer_email'])]
print('total new customers: ', pd.concat([new_cus_2016,new_cus_2017])['customer_email'].nunique())

"""### Lost Customers """

lost_cus_2016 = cus_2015[~cus_2015['customer_email'].isin(cus_2016['customer_email'])]
lost_cus_2017 = cus_2016[~cus_2016['customer_email'].isin(cus_2017['customer_email'])]
print('total lost customers: ', pd.concat([lost_cus_2016,lost_cus_2017])['customer_email'].nunique())

# Calculate retention rate
# (total customer at the end of the period - new customers) / total customer in the begining of the period * 100
(376356 - 374090)/231294 * 100

"""Overall, the company has done a good job in customer acquisition and retention. It has 97% of retention rate and the number of new customers are 1.5 times more than the lost customers."""